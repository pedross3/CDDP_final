{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciência dos Dados - Projeto 03 - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Arthur\\space Alegro \\space de \\space Oliveira$\n",
    "\n",
    "$Pedro\\space dos\\space Santos \\space e \\space Silva$\n",
    "\n",
    "$Jhonata\\space Ferreira\\space de \\space Souza$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from openpyxl import load_workbook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *\n",
    "import warnings\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de dados do ENEM referente aos anos de 2015 e 2016 extraídas pelo site http://portal.inep.gov.br/microdados.\n",
    "\n",
    "Nomes dos arquivos:\n",
    "* MICRODADOS_ENEM_2015.csv\n",
    "* MICRODADOS_ENEM_2016.csv\n",
    "\n",
    "**OBS.:** Para rodar corretamente este arquivo arquivo iPython Notebook (`.ipynb`)  deve-se extrair o arquivo `.zip` correspondente, acessar a pasta \"DADOS\" e colocar os arquivos `.csv` citados anteriormente no mesmo diretório deste arquivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diretório:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\duals\\Documents\\GitHub\\CDDP_final\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link para a base de dados já filtrada:\n",
    "https://drive.google.com/open?id=1yq4DAJyJ2Er902X7Z1JuGc_t7w-aTIXJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Minerando Dados e Características do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando dimensões dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thousand_dot(number):\n",
    "    \"\"\"Convert int value in a string with dots as thousands separator\"\"\"\n",
    "    lista = []\n",
    "    count = 0\n",
    "    for digit in (str(number))[::-1]:\n",
    "        lista.append(digit)\n",
    "        count +=1\n",
    "        if count == 3:\n",
    "            lista.append('.')\n",
    "            count = 0\n",
    "    lista.reverse\n",
    "    elements = ''.join(lista)[::-1]\n",
    "    return elements\n",
    "\n",
    "\n",
    "def dimension_calculator(filename, chunksize=1000, sep=','):\n",
    "    \"\"\"Counts the amount of rows and columns on a .csv file\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "    # Parameters:\n",
    "    rows = 0\n",
    "    columns_labels = ''\n",
    "    count = True\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Processing...\")\n",
    "    # Opening database and counting values:\n",
    "    for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "        rows += len(chunk['NU_INSCRICAO'])\n",
    "        if count == True:\n",
    "            columns_labels = chunk.columns.values\n",
    "            count = False\n",
    "            \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\\n\")   \n",
    "    print(\"Number of rows: {}\".format(thousand_dot(rows)))\n",
    "    print(\"Number of columns: {}\".format(len(columns_labels)))\n",
    "    print(\"Number of elements: {}\".format(thousand_dot(rows*len(columns_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Process complete\n",
      "\n",
      "Number of rows: 7.746.427\n",
      "Number of columns: 166\n",
      "Number of elements: 1.285.906.882\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2015.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete\n",
      "\n",
      "Number of rows: 8.627.367\n",
      "Number of columns: 166\n",
      "Number of elements: 1.432.142.922\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2016.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem do Database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionamos uma parte da base de dados (100.000 rows de dados) para análise em escala reduzida.\n",
    "\n",
    "Após todas as filtragens e cálculos forem concluídos, todo o processo será refeito para cada `chunk`, podendo aplicar as implementações para todo a base de dados muito mais rápido, sem a necessidade de carregar o arquivo completo de uma vez (o que pode ser impossível, pois, em alguns casos, demanda mais processamento e memória que o computador possui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(to_open_filename, database_year,iteration_times, chunksize=1000, sep=','):\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "\n",
    "    # Counters and Status Controllers:\n",
    "    overallcounter = 0\n",
    "    chunkcounter = 0\n",
    "    filecounter = 0\n",
    "    shapecounter = [0, 0]\n",
    "    to_save_filename = \"test_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Opening file...\")\n",
    "    # Loading database in chunks and defining chunk size, correct enconding and reading configs:\n",
    "    for chunk in pd.read_csv(to_open_filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "\n",
    "        # Selecting relevant parameters for dismiss useless data:\n",
    "        chunk = chunk[(chunk[\"TP_PRESENCA_CN\"] == 1) & (chunk[\"TP_PRESENCA_CH\"] == 1) & \n",
    "                      (chunk[\"TP_PRESENCA_LC\"] == 1) & (chunk[\"TP_PRESENCA_MT\"] == 1) & \n",
    "                      (chunk[\"TP_STATUS_REDACAO\"] == 1) & (chunk[\"IN_TREINEIRO\"] == 0)]\n",
    "\n",
    "        # Selecting relevant parameters for fitering data:\n",
    "        chunk = chunk.loc[:, [\"SG_UF_RESIDENCIA\", \"NU_IDADE\", \"TP_SEXO\", \"TP_ESTADO_CIVIL\", \"TP_COR_RACA\", \"TP_ST_CONCLUSAO\", \n",
    "                              \"TP_ESCOLA\", \"NU_NOTA_CN\", \"NU_NOTA_CH\", \"NU_NOTA_LC\", \"NU_NOTA_MT\", \"NU_NOTA_REDACAO\", \"Q006\"]]\n",
    "\n",
    "        # Exporting dataframe to new .csv file below existent data:\n",
    "        chunk.to_csv(to_save_filename, index=False)\n",
    "\n",
    "        # Counters update and process progress exibited to user:\n",
    "        chunkcounter += 1\n",
    "        overallcounter += 1\n",
    "        filecounter += 1\n",
    "        shapecounter[0] += chunk.shape[0]\n",
    "        shapecounter[1] = chunk.shape[1]\n",
    "        print(\"Processing... ({}/{}) - {} rows processed - Saving in '{}')\".format(overallcounter, iteration_times, shapecounter[0], to_save_filename))\n",
    "        to_save_filename = \"test_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    print(\"Process Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "Processing... (1/8) - 723612 rows processed - Saving in 'test_dataframe_2015(0).csv')\n",
      "Processing... (2/8) - 1425321 rows processed - Saving in 'test_dataframe_2015(1).csv')\n",
      "Processing... (3/8) - 2097387 rows processed - Saving in 'test_dataframe_2015(2).csv')\n",
      "Processing... (4/8) - 2742262 rows processed - Saving in 'test_dataframe_2015(3).csv')\n",
      "Processing... (5/8) - 3346414 rows processed - Saving in 'test_dataframe_2015(4).csv')\n",
      "Processing... (6/8) - 3928083 rows processed - Saving in 'test_dataframe_2015(5).csv')\n",
      "Processing... (7/8) - 4429373 rows processed - Saving in 'test_dataframe_2015(6).csv')\n",
      "Processing... (8/8) - 4768463 rows processed - Saving in 'test_dataframe_2015(7).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2015.csv', database_year=2015, chunksize=1000000, iteration_times=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing... (1/9) - 658887 rows processed - Saving in 'test_dataframe_2016(0).csv')\n",
      "Processing... (2/9) - 1309576 rows processed - Saving in 'test_dataframe_2016(1).csv')\n",
      "Processing... (3/9) - 1932155 rows processed - Saving in 'test_dataframe_2016(2).csv')\n",
      "Processing... (4/9) - 2540238 rows processed - Saving in 'test_dataframe_2016(3).csv')\n",
      "Processing... (5/9) - 3116716 rows processed - Saving in 'test_dataframe_2016(4).csv')\n",
      "Processing... (6/9) - 3656028 rows processed - Saving in 'test_dataframe_2016(5).csv')\n",
      "Processing... (7/9) - 4154448 rows processed - Saving in 'test_dataframe_2016(6).csv')\n",
      "Processing... (8/9) - 4604923 rows processed - Saving in 'test_dataframe_2016(7).csv')\n",
      "Processing... (9/9) - 4868906 rows processed - Saving in 'test_dataframe_2016(8).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2016.csv', database_year=2016, chunksize=1000000, iteration_times=9, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando integridade dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 database - Total rows: 4768463\n",
      "2016 database - Total rows: 4868906\n"
     ]
    }
   ],
   "source": [
    "def integrity_verification(filename_list, database_year):\n",
    "    \"\"\"Counts the amount of rows in multiple .csv files\"\"\"\n",
    "    rows_count = 0\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=1000, encoding='latin-1', header=0, sep=','):\n",
    "            rows_count += chunk.shape[0]\n",
    "    print(\"{} database - Total rows: {}\".format(database_year, rows_count))\n",
    "    \n",
    "filename_list_2015 = [\"test_dataframe_2015(0).csv\", \"test_dataframe_2015(1).csv\", \n",
    "                      \"test_dataframe_2015(2).csv\", \"test_dataframe_2015(3).csv\", \n",
    "                      \"test_dataframe_2015(4).csv\", \"test_dataframe_2015(5).csv\", \n",
    "                      \"test_dataframe_2015(6).csv\", \"test_dataframe_2015(7).csv\"]\n",
    "\n",
    "filename_list_2016 = [\"test_dataframe_2016(0).csv\", \"test_dataframe_2016(1).csv\",\n",
    "                      \"test_dataframe_2016(2).csv\", \"test_dataframe_2016(3).csv\", \n",
    "                      \"test_dataframe_2016(4).csv\", \"test_dataframe_2016(5).csv\", \n",
    "                      \"test_dataframe_2016(6).csv\", \"test_dataframe_2016(7).csv\", \n",
    "                      \"test_dataframe_2016(8).csv\"]\n",
    "\n",
    "integrity_verification(filename_list_2015, 2015)\n",
    "integrity_verification(filename_list_2016, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('test8_dataframe.xlsx', encoding='latin-1', header=0)\n",
    "df.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando Bases (teste e treinamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_separation(filename_list, database_year, sep=',', chunksize=1000):\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "            \n",
    "# Creating list with chunk index:\n",
    "indexers = list(database.index)\n",
    "\n",
    "# Embaralhando aleatoriamente os valores:\n",
    "random.shuffle(indexers)\n",
    "\n",
    "# Fazendo slicing de 75% para base de treinamento e 25% para a base de teste\n",
    "training_indexers = indexers[:4181]\n",
    "test_indexers = indexers[4181:]\n",
    "\n",
    "# Ordenando os índices dos databases:\n",
    "training_indexers.sort()\n",
    "test_indexers.sort()\n",
    "\n",
    "# Criando slicing do DataFrame 'database':\n",
    "training_df = database.iloc[training_indexers, :]\n",
    "test_df = database.iloc[test_indexers, :]\n",
    "\n",
    "training_df.is_copy\n",
    "test_df.is_copy\n",
    "# \n",
    "training_df = training_df.reindex(training_indexers)\n",
    "test_df = test_df.reindex(test_indexers)\n",
    "\n",
    "#training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome das colunas:\n",
    "* `SG_UF_RESIDENCIA`: Sigla da Unidade de Federação da residência do candidato\n",
    "* `NU_IDADE`: Idade do candidato\n",
    "* `TP_SEXO`: Sexo do candidato\n",
    "* `TP_ESTADO_CIIVL`: Estado Civil do candidato\n",
    "* `TP_COR_RACA`: Grupo étinco a qual o candidato se identifica\n",
    "* `TP_ST_CONCLUSAO`: Situação de conclusão do Ensino Médio\n",
    "* `TP_ESCOLA`: Tipo de escola do Ensino Médio\n",
    "* `NU_NOTA`: Notas nas avaliações do ENEM, sendo:\n",
    "\n",
    "|Sigla   |    Descrição        |\n",
    "|--------|---------------------|\n",
    "|CN      |Ciências da Natureza |\n",
    "|CH      |Ciências Humanas     |\n",
    "|LC      |Linguagens e Códigos |\n",
    "|MT      |Matemática           |\n",
    "|REDACAO |Redação              |\n",
    "    \n",
    "* `Q006`: Renda mensal da família"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porquê essas colunas foram escolhidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Unidade de Federação é bem relevante no acesso à escolaridade, pois sabe-se que, no Brasil, educação de boa qualidade, de maneira geral, é acessada por meios privados. Sendo assim, há regiões que são pobres em desenvolvimento humano.\n",
    "- Idade influencia, tendo em vista que um jovem de 17-20 anos tende a lembrar mais do conteudo ante a um adulto de 30+\n",
    "- Grupo Étnico (vulgarmente conhecido como \"Raça\") tem grande peso na nota, tendo em vista o sistema de cotas\n",
    "- Situação de conclusão do Ensino Médio diz, aproximadamente, se o candidato se encontra preparado para a prova, ou não\n",
    "- Tipo de escola nos ajuda a montar o perfil do aluno, e  o que esperar do seu desempenho\n",
    "- Nota da prova é útil para a análise pra estabelecer a base de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise descritiva:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gráficos de como cada variável afeta na nota do candidato\n",
    "    - Boxplot de renda com nota\n",
    "    - Gráfico de barras para nota com outra variável qualitativa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites consultados:\n",
    "* https://openpyxl.readthedocs.io/en/2.5/pandas.html\n",
    "* http://portal.inep.gov.br/microdados\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
