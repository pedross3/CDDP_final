{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciência dos Dados - Projeto 03 - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Arthur\\space Alegro \\space de \\space Oliveira$\n",
    "\n",
    "$Pedro\\space dos\\space Santos \\space e \\space Silva$\n",
    "\n",
    "$Jhonata\\space Ferreira\\space de \\space Souza$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "collapsed": true
   },
>>>>>>> e78d72158947f3ab82aea1113d460c14d98c2d83
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from openpyxl import load_workbook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *\n",
    "import warnings\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de dados do ENEM referente aos anos de 2015 e 2016 extraídas pelo site http://portal.inep.gov.br/microdados.\n",
    "\n",
    "Nomes dos arquivos:\n",
    "* MICRODADOS_ENEM_2015.csv\n",
    "* MICRODADOS_ENEM_2016.csv\n",
    "\n",
    "**OBS.:** Para rodar corretamente este arquivo arquivo iPython Notebook (`.ipynb`)  deve-se extrair o arquivo `.zip` correspondente, acessar a pasta \"DADOS\" e colocar os arquivos `.csv` citados anteriormente no mesmo diretório deste arquivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diretório:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\duals\\Documents\\GitHub\\CDDP_final\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link para a base de dados já filtrada:\n",
    "https://drive.google.com/open?id=1yq4DAJyJ2Er902X7Z1JuGc_t7w-aTIXJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Minerando Dados e Características do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando dimensões dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thousand_dot(number):\n",
    "    \"\"\"Convert int value in a string with dots as thousands separator\"\"\"\n",
    "    lista = []\n",
    "    count = 0\n",
    "    for digit in (str(number))[::-1]:\n",
    "        lista.append(digit)\n",
    "        count +=1\n",
    "        if count == 3:\n",
    "            lista.append('.')\n",
    "            count = 0\n",
    "    lista.reverse\n",
    "    elements = ''.join(lista)[::-1]\n",
    "    return elements\n",
    "\n",
    "\n",
    "def dimension_calculator(filename, chunksize=1000, sep=','):\n",
    "    \"\"\"Counts the amount of rows and columns on a .csv file\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "    # Parameters:\n",
    "    rows = 0\n",
    "    columns_labels = ''\n",
    "    count = True\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Processing...\")\n",
    "    # Opening database and counting values:\n",
    "    for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "        rows += len(chunk['NU_INSCRICAO'])\n",
    "        if count == True:\n",
    "            columns_labels = chunk.columns.values\n",
    "            count = False\n",
    "            \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\\n\")   \n",
    "    print(\"Number of rows: {}\".format(thousand_dot(rows)))\n",
    "    print(\"Number of columns: {}\".format(len(columns_labels)))\n",
    "    print(\"Number of elements: {}\".format(thousand_dot(rows*len(columns_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Process complete\n",
      "\n",
      "Number of rows: 7.746.427\n",
      "Number of columns: 166\n",
      "Number of elements: 1.285.906.882\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2015.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete\n",
      "\n",
      "Number of rows: 8.627.367\n",
      "Number of columns: 166\n",
      "Number of elements: 1.432.142.922\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2016.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem do Database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionamos uma parte da base de dados (100.000 rows de dados) para análise em escala reduzida.\n",
    "\n",
    "Após todas as filtragens e cálculos forem concluídos, todo o processo será refeito para cada `chunk`, podendo aplicar as implementações para todo a base de dados muito mais rápido, sem a necessidade de carregar o arquivo completo de uma vez (o que pode ser impossível, pois, em alguns casos, demanda mais processamento e memória que o computador possui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df(to_open_filename, database_year,iteration_times, chunksize=1000, sep=','):\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "\n",
    "    # Counters and Status Controllers:\n",
    "    overallcounter = 0\n",
    "    chunkcounter = 0\n",
    "    filecounter = 0\n",
    "    shapecounter = [0, 0]\n",
    "    to_save_filename = \"filtered_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Opening file...\")\n",
    "    # Loading database in chunks and defining chunk size, correct enconding and reading configs:\n",
    "    for chunk in pd.read_csv(to_open_filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "\n",
    "        # Selecting relevant parameters for dismiss useless data:\n",
    "        chunk = chunk[(chunk[\"TP_PRESENCA_CN\"] == 1) & (chunk[\"TP_PRESENCA_CH\"] == 1) & \n",
    "                      (chunk[\"TP_PRESENCA_LC\"] == 1) & (chunk[\"TP_PRESENCA_MT\"] == 1) & \n",
    "                      (chunk[\"TP_STATUS_REDACAO\"] == 1) & (chunk[\"IN_TREINEIRO\"] == 0)]\n",
    "\n",
    "        # Selecting relevant parameters for fitering data:\n",
    "        chunk = chunk.loc[:, [\"SG_UF_RESIDENCIA\", \"NU_IDADE\", \"TP_SEXO\", \"TP_ESTADO_CIVIL\", \"TP_COR_RACA\", \"TP_ST_CONCLUSAO\", \n",
    "                              \"TP_ESCOLA\", \"NU_NOTA_CN\", \"NU_NOTA_CH\", \"NU_NOTA_LC\", \"NU_NOTA_MT\", \"NU_NOTA_REDACAO\", \"Q006\"]]\n",
    "\n",
    "        # Exporting dataframe to new .csv file below existent data:\n",
    "        chunk.to_csv(to_save_filename, index=False)\n",
    "\n",
    "        # Counters update and process progress exibited to user:\n",
    "        chunkcounter += 1\n",
    "        overallcounter += 1\n",
    "        filecounter += 1\n",
    "        shapecounter[0] += chunk.shape[0]\n",
    "        shapecounter[1] = chunk.shape[1]\n",
    "        print(\"Processing... ({}/{}) - {} rows processed - Saving in '{}')\".format(overallcounter, iteration_times, shapecounter[0], to_save_filename))\n",
    "        to_save_filename = \"filtered_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    print(\"Process Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "Processing... (1/8) - 723612 rows processed - Saving in 'filtered_dataframe_2015(0).csv')\n",
      "Processing... (2/8) - 1425321 rows processed - Saving in 'filtered_dataframe_2015(1).csv')\n",
      "Processing... (3/8) - 2097387 rows processed - Saving in 'filtered_dataframe_2015(2).csv')\n",
      "Processing... (4/8) - 2742262 rows processed - Saving in 'filtered_dataframe_2015(3).csv')\n",
      "Processing... (5/8) - 3346414 rows processed - Saving in 'filtered_dataframe_2015(4).csv')\n",
      "Processing... (6/8) - 3928083 rows processed - Saving in 'filtered_dataframe_2015(5).csv')\n",
      "Processing... (7/8) - 4429373 rows processed - Saving in 'filtered_dataframe_2015(6).csv')\n",
      "Processing... (8/8) - 4768463 rows processed - Saving in 'filtered_dataframe_2015(7).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2015.csv', database_year=2015, chunksize=1000000, iteration_times=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "Processing... (1/9) - 658887 rows processed - Saving in 'filtered_dataframe_2016(0).csv')\n",
      "Processing... (2/9) - 1309576 rows processed - Saving in 'filtered_dataframe_2016(1).csv')\n",
      "Processing... (3/9) - 1932155 rows processed - Saving in 'filtered_dataframe_2016(2).csv')\n",
      "Processing... (4/9) - 2540238 rows processed - Saving in 'filtered_dataframe_2016(3).csv')\n",
      "Processing... (5/9) - 3116716 rows processed - Saving in 'filtered_dataframe_2016(4).csv')\n",
      "Processing... (6/9) - 3656028 rows processed - Saving in 'filtered_dataframe_2016(5).csv')\n",
      "Processing... (7/9) - 4154448 rows processed - Saving in 'filtered_dataframe_2016(6).csv')\n",
      "Processing... (8/9) - 4604923 rows processed - Saving in 'filtered_dataframe_2016(7).csv')\n",
      "Processing... (9/9) - 4868906 rows processed - Saving in 'filtered_dataframe_2016(8).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2016.csv', database_year=2016, chunksize=1000000, iteration_times=9, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando integridade dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
=======
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
>>>>>>> e78d72158947f3ab82aea1113d460c14d98c2d83
   "outputs": [],
   "source": [
    "filename_list_2015 = [\"filtered_dataframe_2015(0).csv\", \"filtered_dataframe_2015(1).csv\", \n",
    "                      \"filtered_dataframe_2015(2).csv\", \"filtered_dataframe_2015(3).csv\", \n",
    "                      \"filtered_dataframe_2015(4).csv\", \"filtered_dataframe_2015(5).csv\", \n",
    "                      \"filtered_dataframe_2015(6).csv\", \"filtered_dataframe_2015(7).csv\"]\n",
    "\n",
    "filename_list_2016 = [\"filtered_dataframe_2016(0).csv\", \"filtered_dataframe_2016(1).csv\",\n",
    "                      \"filtered_dataframe_2016(2).csv\", \"filtered_dataframe_2016(3).csv\", \n",
    "                      \"filtered_dataframe_2016(4).csv\", \"filtered_dataframe_2016(5).csv\", \n",
    "                      \"filtered_dataframe_2016(6).csv\", \"filtered_dataframe_2016(7).csv\", \n",
    "                      \"filtered_dataframe_2016(8).csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def integrity_verification(filename_list, database_year):\n",
    "    \"\"\"Counts the amount of rows in multiple .csv files\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on files amount and/or files size this process might take time to compute\")\n",
    "    rows_count = 0\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=1000, encoding='latin-1', header=0, sep=','):\n",
    "            rows_count += chunk.shape[0]\n",
    "    print(\"{} database - Total rows: {}\".format(database_year, rows_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on files amount and/or files size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 database - Total rows: 4768463\n",
      "2016 database - Total rows: 4868906\n"
     ]
    }
   ],
   "source": [
    "integrity_verification(filename_list_2015, 2015)\n",
    "integrity_verification(filename_list_2016, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_multifiles(filename_list, database_year):\n",
    "    \"\"\"Merging all data to unique filtered file\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\")\n",
    "\n",
    "    print(\"Process iniciated...\")\n",
    "    #\n",
    "    chunk_list = []\n",
    "    \n",
    "    print(\"Processing...\")\n",
    "    \n",
    "    for filename in filename_list:\n",
    "            for chunk in pd.read_csv(filename, chunksize=10000, encoding='latin-1', header=0, sep=','):\n",
    "                chunk_list.append(chunk)\n",
    "                \n",
    "    print(\"Merging all data...\")\n",
    "    final_df = pd.concat(chunk_list)\n",
    "    \n",
    "    print(\"Exporting data...\")\n",
    "    final_df.to_csv(\"filtered_unique_database_{}.csv\".format(database_year), index=False)\n",
    "    print(\"Process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "Processing...\n",
      "Merging all data...\n",
      "Exporting data...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "merge_multifiles(filename_list_2015, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all data...\n",
      "Exporting data...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "merge_multifiles(filename_list_2016, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando Bases (teste e treinamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def database_separation(filename_list, database_year, display_step=10, sep=',', chunksize=1000):\n",
    "    \"\"\"Generates random sample for training and test databases and exports to separated .csv files\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\")\n",
    "    # Local data storages:\n",
    "    training_df_list = []\n",
    "    test_df_list = []\n",
    "    # Counter:\n",
    "    chunkcounter = 0\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process iniciated...\")\n",
    "    # Reading all data from filtered .csv files:\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "            # Creating list with chunk indexers:\n",
    "            indexers = list(range(0, (chunksize + 1)))\n",
    "            \n",
    "            #\n",
    "            chunk.reindex(indexers)\n",
    "            \n",
    "            # Random shuffling chunk's index values:\n",
    "            random.shuffle(indexers)\n",
    "            \n",
    "            # Separating 80% to training database and 20% to test database:\n",
    "            training_chunk_idx = indexers[:int((0.8)*chunksize)]\n",
    "            test_chunk_idx = indexers[int((0.8)*chunksize):]\n",
    "\n",
    "            # Sorting chunk indexers:\n",
    "            training_chunk_idx.sort()\n",
    "            test_chunk_idx.sort()\n",
    "            \n",
    "            # Slicing chunk's data:\n",
    "            training_chunk = chunk.iloc[training_chunk_idx, :]\n",
    "            test_chunk = chunk.iloc[test_chunk_idx, :]\n",
    "\n",
    "            # Setting 'is_copy' argument (avoid file corruption):\n",
    "            training_chunk.is_copy\n",
    "            test_chunk.is_copy\n",
    "            \n",
    "            # Appending separated chunk data to reference DataFrame list:\n",
    "            training_df_list.append(training_chunk)\n",
    "            test_df_list.append(test_chunk)\n",
    "            \n",
    "            # Counters update and process progress exibited to user:\n",
    "            chunkcounter += 1\n",
    "            \n",
    "            if chunkcounter%display_step == 0:\n",
    "                print(\"{} chunks processed - {} rows processed\".format(chunkcounter, chunkcounter*chunksize))\n",
    "                \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Meriging all data...\")\n",
    "    # Merging to unique dataframe:\n",
    "    training_df = pd.concat(training_df_list, ignore_index=True)\n",
    "    test_df = pd.concat(test_df_list, ignore_index=True)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Exporting data...\")\n",
    "    # Exporting DataFrames to separated .csv files:\n",
    "    #training_df.to_csv(\"training_database_{}.csv\".format(database_year), index=False)\n",
    "    test_df.to_csv(\"test_database_{}.csv\".format(database_year), index=False)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_separation(filename_list, database_year, display_step=10, sep=',', chunksize=1000):\n",
    "    \"\"\"Generates random sample for training and test databases and exports to separated .csv files\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\")\n",
    "    # Local data storages:\n",
    "    training_df_list = []\n",
    "    test_df_list = []\n",
    "    # Counter:\n",
    "    chunkcounter = 0\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process iniciated...\")\n",
    "    # Reading all data from filtered .csv files:\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "            # Generating a random sample:\n",
    "            training_chunk = chunk.sample(frac=0.8)\n",
    "            \n",
    "            indexers = training_chunk.index\n",
    "            \n",
    "            test_chunk = chunk[~chunk.index.isin(indexers)]\n",
    "            \n",
    "            training_df_list.append(training_chunk)\n",
    "            test_df_list.append(test_chunk)\n",
    "            \n",
    "                \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Meriging all data...\")\n",
    "    # Merging to unique dataframe:\n",
    "    training_df = pd.concat(training_df_list, ignore_index=True)\n",
    "    test_df = pd.concat(test_df_list, ignore_index=True)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Exporting data...\")\n",
    "    # Exporting DataFrames to separated .csv files:\n",
    "    #training_df.to_csv(\"training_database_{}.csv\".format(database_year), index=False)\n",
    "    test_df.to_csv(\"test_database_{}.csv\".format(database_year), index=False)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "0 chunks processed - 0 rows processed\n",
      "Meriging all data...\n",
      "Exporting data...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "database_separation(filename_list_2015, 2015, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reindex in module pandas.core.frame:\n",
      "\n",
      "reindex(self, labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None)\n",
      "    Conform DataFrame to new index with optional filling logic, placing\n",
      "    NA/NaN in locations having no value in the previous index. A new object\n",
      "    is produced unless the new index is equivalent to the current one and\n",
      "    copy=False\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    labels : array-like, optional\n",
      "        New labels / index to conform the axis specified by 'axis' to.\n",
      "    index, columns : array-like, optional (should be specified using keywords)\n",
      "        New labels / index to conform to. Preferably an Index object to\n",
      "        avoid duplicating data\n",
      "    axis : int or str, optional\n",
      "        Axis to target. Can be either the axis name ('index', 'columns')\n",
      "        or number (0, 1).\n",
      "    method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}, optional\n",
      "        method to use for filling holes in reindexed DataFrame.\n",
      "        Please note: this is only  applicable to DataFrames/Series with a\n",
      "        monotonically increasing/decreasing index.\n",
      "    \n",
      "        * default: don't fill gaps\n",
      "        * pad / ffill: propagate last valid observation forward to next\n",
      "          valid\n",
      "        * backfill / bfill: use next valid observation to fill gap\n",
      "        * nearest: use nearest valid observations to fill gap\n",
      "    \n",
      "    copy : boolean, default True\n",
      "        Return a new object, even if the passed indexes are the same\n",
      "    level : int or name\n",
      "        Broadcast across a level, matching Index values on the\n",
      "        passed MultiIndex level\n",
      "    fill_value : scalar, default np.NaN\n",
      "        Value to use for missing values. Defaults to NaN, but can be any\n",
      "        \"compatible\" value\n",
      "    limit : int, default None\n",
      "        Maximum number of consecutive elements to forward or backward fill\n",
      "    tolerance : optional\n",
      "        Maximum distance between original and new labels for inexact\n",
      "        matches. The values of the index at the matching locations most\n",
      "        satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      "    \n",
      "        Tolerance may be a scalar value, which applies the same tolerance\n",
      "        to all values, or list-like, which applies variable tolerance per\n",
      "        element. List-like includes list, tuple, array, Series, and must be\n",
      "        the same size as the index and its dtype must exactly match the\n",
      "        index's type.\n",
      "    \n",
      "        .. versionadded:: 0.17.0\n",
      "        .. versionadded:: 0.21.0 (list-like tolerance)\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    ``DataFrame.reindex`` supports two calling conventions\n",
      "    \n",
      "    * ``(index=index_labels, columns=column_labels, ...)``\n",
      "    * ``(labels, axis={'index', 'columns'}, ...)``\n",
      "    \n",
      "    We *highly* recommend using keyword arguments to clarify your\n",
      "    intent.\n",
      "    \n",
      "    Create a dataframe with some fictional data.\n",
      "    \n",
      "    >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n",
      "    >>> df = pd.DataFrame({\n",
      "    ...      'http_status': [200,200,404,404,301],\n",
      "    ...      'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n",
      "    ...       index=index)\n",
      "    >>> df\n",
      "               http_status  response_time\n",
      "    Firefox            200           0.04\n",
      "    Chrome             200           0.02\n",
      "    Safari             404           0.07\n",
      "    IE10               404           0.08\n",
      "    Konqueror          301           1.00\n",
      "    \n",
      "    Create a new index and reindex the dataframe. By default\n",
      "    values in the new index that do not have corresponding\n",
      "    records in the dataframe are assigned ``NaN``.\n",
      "    \n",
      "    >>> new_index= ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n",
      "    ...             'Chrome']\n",
      "    >>> df.reindex(new_index)\n",
      "                   http_status  response_time\n",
      "    Safari               404.0           0.07\n",
      "    Iceweasel              NaN            NaN\n",
      "    Comodo Dragon          NaN            NaN\n",
      "    IE10                 404.0           0.08\n",
      "    Chrome               200.0           0.02\n",
      "    \n",
      "    We can fill in the missing values by passing a value to\n",
      "    the keyword ``fill_value``. Because the index is not monotonically\n",
      "    increasing or decreasing, we cannot use arguments to the keyword\n",
      "    ``method`` to fill the ``NaN`` values.\n",
      "    \n",
      "    >>> df.reindex(new_index, fill_value=0)\n",
      "                   http_status  response_time\n",
      "    Safari                 404           0.07\n",
      "    Iceweasel                0           0.00\n",
      "    Comodo Dragon            0           0.00\n",
      "    IE10                   404           0.08\n",
      "    Chrome                 200           0.02\n",
      "    \n",
      "    >>> df.reindex(new_index, fill_value='missing')\n",
      "                  http_status response_time\n",
      "    Safari                404          0.07\n",
      "    Iceweasel         missing       missing\n",
      "    Comodo Dragon     missing       missing\n",
      "    IE10                  404          0.08\n",
      "    Chrome                200          0.02\n",
      "    \n",
      "    We can also reindex the columns.\n",
      "    \n",
      "    >>> df.reindex(columns=['http_status', 'user_agent'])\n",
      "               http_status  user_agent\n",
      "    Firefox            200         NaN\n",
      "    Chrome             200         NaN\n",
      "    Safari             404         NaN\n",
      "    IE10               404         NaN\n",
      "    Konqueror          301         NaN\n",
      "    \n",
      "    Or we can use \"axis-style\" keyword arguments\n",
      "    \n",
      "    >>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n",
      "               http_status  user_agent\n",
      "    Firefox            200         NaN\n",
      "    Chrome             200         NaN\n",
      "    Safari             404         NaN\n",
      "    IE10               404         NaN\n",
      "    Konqueror          301         NaN\n",
      "    \n",
      "    To further illustrate the filling functionality in\n",
      "    ``reindex``, we will create a dataframe with a\n",
      "    monotonically increasing index (for example, a sequence\n",
      "    of dates).\n",
      "    \n",
      "    >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n",
      "    >>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n",
      "    ...                    index=date_index)\n",
      "    >>> df2\n",
      "                prices\n",
      "    2010-01-01     100\n",
      "    2010-01-02     101\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04     100\n",
      "    2010-01-05      89\n",
      "    2010-01-06      88\n",
      "    \n",
      "    Suppose we decide to expand the dataframe to cover a wider\n",
      "    date range.\n",
      "    \n",
      "    >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n",
      "    >>> df2.reindex(date_index2)\n",
      "                prices\n",
      "    2009-12-29     NaN\n",
      "    2009-12-30     NaN\n",
      "    2009-12-31     NaN\n",
      "    2010-01-01     100\n",
      "    2010-01-02     101\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04     100\n",
      "    2010-01-05      89\n",
      "    2010-01-06      88\n",
      "    2010-01-07     NaN\n",
      "    \n",
      "    The index entries that did not have a value in the original data frame\n",
      "    (for example, '2009-12-29') are by default filled with ``NaN``.\n",
      "    If desired, we can fill in the missing values using one of several\n",
      "    options.\n",
      "    \n",
      "    For example, to backpropagate the last valid value to fill the ``NaN``\n",
      "    values, pass ``bfill`` as an argument to the ``method`` keyword.\n",
      "    \n",
      "    >>> df2.reindex(date_index2, method='bfill')\n",
      "                prices\n",
      "    2009-12-29     100\n",
      "    2009-12-30     100\n",
      "    2009-12-31     100\n",
      "    2010-01-01     100\n",
      "    2010-01-02     101\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04     100\n",
      "    2010-01-05      89\n",
      "    2010-01-06      88\n",
      "    2010-01-07     NaN\n",
      "    \n",
      "    Please note that the ``NaN`` value present in the original dataframe\n",
      "    (at index value 2010-01-03) will not be filled by any of the\n",
      "    value propagation schemes. This is because filling while reindexing\n",
      "    does not look at dataframe values, but only compares the original and\n",
      "    desired indexes. If you do want to fill in the ``NaN`` values present\n",
      "    in the original dataframe, use the ``fillna()`` method.\n",
      "    \n",
      "    See the :ref:`user guide <basics.reindexing>` for more.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    reindexed : DataFrame\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.reindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "Meriging all data...\n",
      "Exporting data...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "database_separation(filename_list_2016, 2016, chunksize=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome das colunas:\n",
    "* `SG_UF_RESIDENCIA`: Sigla da Unidade de Federação da residência do candidato\n",
    "* `NU_IDADE`: Idade do candidato\n",
    "* `TP_SEXO`: Sexo do candidato\n",
    "* `TP_ESTADO_CIIVL`: Estado Civil do candidato\n",
    "* `TP_COR_RACA`: Grupo étinco a qual o candidato se identifica\n",
    "* `TP_ST_CONCLUSAO`: Situação de conclusão do Ensino Médio\n",
    "* `TP_ESCOLA`: Tipo de escola do Ensino Médio\n",
    "* `NU_NOTA`: Notas nas avaliações do ENEM, sendo:\n",
    "\n",
    "|Sigla   |    Descrição        |\n",
    "|--------|---------------------|\n",
    "|CN      |Ciências da Natureza |\n",
    "|CH      |Ciências Humanas     |\n",
    "|LC      |Linguagens e Códigos |\n",
    "|MT      |Matemática           |\n",
    "|REDACAO |Redação              |\n",
    "    \n",
    "* `Q006`: Renda mensal da família"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porquê essas colunas foram escolhidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Unidade de Federação é bem relevante no acesso à escolaridade, pois sabe-se que, no Brasil, educação de boa qualidade, de maneira geral, é acessada por meios privados. Sendo assim, há regiões que são pobres em desenvolvimento humano.\n",
    "- Idade influencia, tendo em vista que um jovem de 17-20 anos tende a lembrar mais do conteudo ante a um adulto de 30+\n",
    "- Grupo Étnico (vulgarmente conhecido como \"Raça\") tem grande peso na nota, tendo em vista o sistema de cotas\n",
    "- Situação de conclusão do Ensino Médio diz, aproximadamente, se o candidato se encontra preparado para a prova, ou não\n",
    "- Tipo de escola nos ajuda a montar o perfil do aluno, e  o que esperar do seu desempenho\n",
    "- Nota da prova é útil para a análise pra estabelecer a base de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise descritiva:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gráficos de como cada variável afeta na nota do candidato\n",
    "    - Boxplot de renda com nota\n",
    "    - Gráfico de barras para nota com outra variável qualitativa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2015 = pd.read_csv('filtered_unique_database_2015.csv', encoding='latin-1', header=0, sep=',')\n",
    "\n",
    "data_2016 = pd.read_csv('filtered_unique_database_2016.csv', encoding='latin-1', header=0, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2015n = pd.read_csv('filtered_dataframe_2015(0).csv', encoding='latin-1', header=0, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SG_UF_RESIDENCIA</th>\n",
       "      <th>NU_IDADE</th>\n",
       "      <th>TP_SEXO</th>\n",
       "      <th>TP_ESTADO_CIVIL</th>\n",
       "      <th>TP_COR_RACA</th>\n",
       "      <th>TP_ST_CONCLUSAO</th>\n",
       "      <th>TP_ESCOLA</th>\n",
       "      <th>NU_NOTA_CN</th>\n",
       "      <th>NU_NOTA_CH</th>\n",
       "      <th>NU_NOTA_LC</th>\n",
       "      <th>NU_NOTA_MT</th>\n",
       "      <th>NU_NOTA_REDACAO</th>\n",
       "      <th>Q006</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RS</td>\n",
       "      <td>42.0</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>657.4</td>\n",
       "      <td>705.3</td>\n",
       "      <td>591.1</td>\n",
       "      <td>732.3</td>\n",
       "      <td>760.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PE</td>\n",
       "      <td>22.0</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>528.5</td>\n",
       "      <td>531.1</td>\n",
       "      <td>511.3</td>\n",
       "      <td>566.5</td>\n",
       "      <td>640.0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MG</td>\n",
       "      <td>18.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>679.9</td>\n",
       "      <td>730.6</td>\n",
       "      <td>621.2</td>\n",
       "      <td>732.4</td>\n",
       "      <td>800.0</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SC</td>\n",
       "      <td>19.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>598.7</td>\n",
       "      <td>658.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>576.4</td>\n",
       "      <td>480.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SP</td>\n",
       "      <td>17.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>652.1</td>\n",
       "      <td>671.9</td>\n",
       "      <td>609.3</td>\n",
       "      <td>685.5</td>\n",
       "      <td>820.0</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SG_UF_RESIDENCIA  NU_IDADE TP_SEXO  TP_ESTADO_CIVIL  TP_COR_RACA  \\\n",
       "0               RS      42.0       M              1.0            1   \n",
       "1               PE      22.0       M              1.0            2   \n",
       "2               MG      18.0       M              0.0            3   \n",
       "3               SC      19.0       M              0.0            1   \n",
       "4               SP      17.0       F              0.0            1   \n",
       "\n",
       "   TP_ST_CONCLUSAO  TP_ESCOLA  NU_NOTA_CN  NU_NOTA_CH  NU_NOTA_LC  NU_NOTA_MT  \\\n",
       "0                1          1       657.4       705.3       591.1       732.3   \n",
       "1                4          1       528.5       531.1       511.3       566.5   \n",
       "2                1          1       679.9       730.6       621.2       732.4   \n",
       "3                1          1       598.7       658.0       579.0       576.4   \n",
       "4                2          2       652.1       671.9       609.3       685.5   \n",
       "\n",
       "   NU_NOTA_REDACAO Q006  \n",
       "0            760.0    D  \n",
       "1            640.0    B  \n",
       "2            800.0    G  \n",
       "3            480.0    F  \n",
       "4            820.0    J  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erro bosta que fudeu tudo que eu fiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "In draw_path: Exceeded cell block limit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'png'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'retina'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'png2x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[0;32m   2190\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2191\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2192\u001b[1;33m                     **kwargs)\n\u001b[0m\u001b[0;32m   2193\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2194\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[0mRendererAgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[1;32m-> 1143\u001b[1;33m                 renderer, self, dsu, self.suppressComposite)\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'figure'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, dsu, suppress_composite)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mzorder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdsu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, inframe)\u001b[0m\n\u001b[0;32m   2407\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2409\u001b[1;33m         \u001b[0mmimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2411\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, dsu, suppress_composite)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mzorder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdsu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    820\u001b[0m                     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_sketch_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_sketch_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                 \u001b[0mdrawFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36m_draw_lines\u001b[1;34m(self, renderer, gc, path, trans)\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_draw_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lineFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_draw_steps_pre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36m_draw_solid\u001b[1;34m(self, renderer, gc, path, trans)\u001b[0m\n\u001b[0;32m   1291\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_linestyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'solid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1292\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_dashes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dashOffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dashSeq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1293\u001b[1;33m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_draw_dashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mdraw_path\u001b[1;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgbFace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgbFace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdraw_mathtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: In draw_path: Exceeded cell block limit"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21a851ec4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_2015n.NU_NOTA_CN, data_2015n.NU_NOTA_MT)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites consultados:\n",
    "* https://openpyxl.readthedocs.io/en/2.5/pandas.html\n",
    "* http://portal.inep.gov.br/microdados\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
