{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciência dos Dados - Projeto 03 - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Arthur\\space Alegro \\space de \\space Oliveira$\n",
    "\n",
    "$Pedro\\space dos\\space Santos \\space e \\space Silva$\n",
    "\n",
    "$Jhonata\\space Ferreira\\space de \\space Souza$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from openpyxl import load_workbook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *\n",
    "import warnings\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de dados do ENEM referente aos anos de 2015 e 2016 extraídas pelo site http://portal.inep.gov.br/microdados.\n",
    "\n",
    "Nomes dos arquivos:\n",
    "* MICRODADOS_ENEM_2015.csv\n",
    "* MICRODADOS_ENEM_2016.csv\n",
    "\n",
    "**OBS.:** Para rodar corretamente este arquivo arquivo iPython Notebook (`.ipynb`)  deve-se extrair o arquivo `.zip` correspondente, acessar a pasta \"DADOS\" e colocar os arquivos `.csv` citados anteriormente no mesmo diretório deste arquivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diretório:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\duals\\Documents\\GitHub\\CDDP_final\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link para a base de dados já filtrada:\n",
    "https://drive.google.com/open?id=1yq4DAJyJ2Er902X7Z1JuGc_t7w-aTIXJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Minerando Dados e Características do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando dimensões dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thousand_dot(number):\n",
    "    \"\"\"Convert int value in a string with dots as thousands separator\"\"\"\n",
    "    lista = []\n",
    "    count = 0\n",
    "    for digit in (str(number))[::-1]:\n",
    "        lista.append(digit)\n",
    "        count +=1\n",
    "        if count == 3:\n",
    "            lista.append('.')\n",
    "            count = 0\n",
    "    lista.reverse\n",
    "    elements = ''.join(lista)[::-1]\n",
    "    return elements\n",
    "\n",
    "\n",
    "def dimension_calculator(filename, chunksize=1000, sep=','):\n",
    "    \"\"\"Counts the amount of rows and columns on a .csv file\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "    # Parameters:\n",
    "    rows = 0\n",
    "    columns_labels = ''\n",
    "    count = True\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Processing...\")\n",
    "    # Opening database and counting values:\n",
    "    for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "        rows += len(chunk['NU_INSCRICAO'])\n",
    "        if count == True:\n",
    "            columns_labels = chunk.columns.values\n",
    "            count = False\n",
    "            \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\\n\")   \n",
    "    print(\"Number of rows: {}\".format(thousand_dot(rows)))\n",
    "    print(\"Number of columns: {}\".format(len(columns_labels)))\n",
    "    print(\"Number of elements: {}\".format(thousand_dot(rows*len(columns_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Process complete\n",
      "\n",
      "Number of rows: 7.746.427\n",
      "Number of columns: 166\n",
      "Number of elements: 1.285.906.882\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2015.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete\n",
      "\n",
      "Number of rows: 8.627.367\n",
      "Number of columns: 166\n",
      "Number of elements: 1.432.142.922\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2016.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem do Database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionamos uma parte da base de dados (100.000 rows de dados) para análise em escala reduzida.\n",
    "\n",
    "Após todas as filtragens e cálculos forem concluídos, todo o processo será refeito para cada `chunk`, podendo aplicar as implementações para todo a base de dados muito mais rápido, sem a necessidade de carregar o arquivo completo de uma vez (o que pode ser impossível, pois, em alguns casos, demanda mais processamento e memória que o computador possui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(to_open_filename, database_year,iteration_times, chunksize=1000, sep=','):\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "\n",
    "    # Counters and Status Controllers:\n",
    "    overallcounter = 0\n",
    "    chunkcounter = 0\n",
    "    filecounter = 0\n",
    "    shapecounter = [0, 0]\n",
    "    to_save_filename = \"test_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Opening file...\")\n",
    "    # Loading database in chunks and defining chunk size, correct enconding and reading configs:\n",
    "    for chunk in pd.read_csv(to_open_filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "\n",
    "        # Selecting relevant parameters for dismiss useless data:\n",
    "        chunk = chunk[(chunk[\"TP_PRESENCA_CN\"] == 1) & (chunk[\"TP_PRESENCA_CH\"] == 1) & \n",
    "                      (chunk[\"TP_PRESENCA_LC\"] == 1) & (chunk[\"TP_PRESENCA_MT\"] == 1) & \n",
    "                      (chunk[\"TP_STATUS_REDACAO\"] == 1) & (chunk[\"IN_TREINEIRO\"] == 0)]\n",
    "\n",
    "        # Selecting relevant parameters for fitering data:\n",
    "        chunk = chunk.loc[:, [\"SG_UF_RESIDENCIA\", \"NU_IDADE\", \"TP_SEXO\", \"TP_ESTADO_CIVIL\", \"TP_COR_RACA\", \"TP_ST_CONCLUSAO\", \n",
    "                              \"TP_ESCOLA\", \"NU_NOTA_CN\", \"NU_NOTA_CH\", \"NU_NOTA_LC\", \"NU_NOTA_MT\", \"NU_NOTA_REDACAO\", \"Q006\"]]\n",
    "\n",
    "        # Exporting dataframe to new .csv file below existent data:\n",
    "        chunk.to_csv(to_save_filename, index=False)\n",
    "\n",
    "        # Counters update and process progress exibited to user:\n",
    "        chunkcounter += 1\n",
    "        overallcounter += 1\n",
    "        filecounter += 1\n",
    "        shapecounter[0] += chunk.shape[0]\n",
    "        shapecounter[1] = chunk.shape[1]\n",
    "        print(\"Processing... ({}/{}) - {} rows processed - Saving in '{}')\".format(overallcounter, iteration_times, shapecounter[0], to_save_filename))\n",
    "        to_save_filename = \"test_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    print(\"Process Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "Processing... (1/8) - 723612 rows processed - Saving in 'test_dataframe_2015(0).csv')\n",
      "Processing... (2/8) - 1425321 rows processed - Saving in 'test_dataframe_2015(1).csv')\n",
      "Processing... (3/8) - 2097387 rows processed - Saving in 'test_dataframe_2015(2).csv')\n",
      "Processing... (4/8) - 2742262 rows processed - Saving in 'test_dataframe_2015(3).csv')\n",
      "Processing... (5/8) - 3346414 rows processed - Saving in 'test_dataframe_2015(4).csv')\n",
      "Processing... (6/8) - 3928083 rows processed - Saving in 'test_dataframe_2015(5).csv')\n",
      "Processing... (7/8) - 4429373 rows processed - Saving in 'test_dataframe_2015(6).csv')\n",
      "Processing... (8/8) - 4768463 rows processed - Saving in 'test_dataframe_2015(7).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2015.csv', database_year=2015, chunksize=1000000, iteration_times=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing... (1/9) - 658887 rows processed - Saving in 'test_dataframe_2016(0).csv')\n",
      "Processing... (2/9) - 1309576 rows processed - Saving in 'test_dataframe_2016(1).csv')\n",
      "Processing... (3/9) - 1932155 rows processed - Saving in 'test_dataframe_2016(2).csv')\n",
      "Processing... (4/9) - 2540238 rows processed - Saving in 'test_dataframe_2016(3).csv')\n",
      "Processing... (5/9) - 3116716 rows processed - Saving in 'test_dataframe_2016(4).csv')\n",
      "Processing... (6/9) - 3656028 rows processed - Saving in 'test_dataframe_2016(5).csv')\n",
      "Processing... (7/9) - 4154448 rows processed - Saving in 'test_dataframe_2016(6).csv')\n",
      "Processing... (8/9) - 4604923 rows processed - Saving in 'test_dataframe_2016(7).csv')\n",
      "Processing... (9/9) - 4868906 rows processed - Saving in 'test_dataframe_2016(8).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2016.csv', database_year=2016, chunksize=1000000, iteration_times=9, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando integridade dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list_2015 = [\"test_dataframe_2015(0).csv\", \"test_dataframe_2015(1).csv\", \n",
    "                      \"test_dataframe_2015(2).csv\", \"test_dataframe_2015(3).csv\", \n",
    "                      \"test_dataframe_2015(4).csv\", \"test_dataframe_2015(5).csv\", \n",
    "                      \"test_dataframe_2015(6).csv\", \"test_dataframe_2015(7).csv\"]\n",
    "\n",
    "filename_list_2016 = [\"test_dataframe_2016(0).csv\", \"test_dataframe_2016(1).csv\",\n",
    "                      \"test_dataframe_2016(2).csv\", \"test_dataframe_2016(3).csv\", \n",
    "                      \"test_dataframe_2016(4).csv\", \"test_dataframe_2016(5).csv\", \n",
    "                      \"test_dataframe_2016(6).csv\", \"test_dataframe_2016(7).csv\", \n",
    "                      \"test_dataframe_2016(8).csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrity_verification(filename_list, database_year):\n",
    "    \"\"\"Counts the amount of rows in multiple .csv files\"\"\"\n",
    "    rows_count = 0\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=1000, encoding='latin-1', header=0, sep=','):\n",
    "            rows_count += chunk.shape[0]\n",
    "    print(\"{} database - Total rows: {}\".format(database_year, rows_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 database - Total rows: 4768463\n",
      "2016 database - Total rows: 4868906\n"
     ]
    }
   ],
   "source": [
    "integrity_verification(filename_list_2015, 2015)\n",
    "integrity_verification(filename_list_2016, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando Bases (teste e treinamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_separation(filename_list, database_year, display_step=10, sep=',', chunksize=1000):\n",
    "    \"\"\"Generates random sample for training and test databases and exports to separated .csv files\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\")\n",
    "    # Local data storages:\n",
    "    training_df_list = []\n",
    "    test_df_list = []\n",
    "    # Counter:\n",
    "    chunkcounter = 0\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process iniciated...\")\n",
    "    # Reading all data from filtered .csv files:\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "            # Creating list with chunk indexers:\n",
    "            indexers = list(chunk.index)\n",
    "            \n",
    "            # Random shuffling chunk's index values:\n",
    "            random.shuffle(indexers)\n",
    "            \n",
    "            # Separating 80% to training database and 20% to test database:\n",
    "            training_chunk_idx = indexers[:int((0.8)*chunksize)]\n",
    "            test_chunk_idx = indexers[int((0.8)*chunksize):]\n",
    "\n",
    "            # Sorting chunk indexers:\n",
    "            training_chunk_idx.sort()\n",
    "            test_chunk_idx.sort()\n",
    "            print(len(training_chunk_idx))\n",
    "            print(len(test_chunk_idx))\n",
    "            # Slicing chunk's data:\n",
    "            training_chunk = chunk.iloc[training_chunk_idx, :]\n",
    "            test_chunk = chunk.iloc[test_chunk_idx, :]\n",
    "\n",
    "            # Setting 'is_copy' argument (avoid file corruption):\n",
    "            training_chunk.is_copy\n",
    "            test_chunk.is_copy\n",
    "            \n",
    "            # Appending separated chunk data to reference DataFrame list:\n",
    "            training_df_list.append(training_chunk)\n",
    "            test_df_list.append(test_chunk)\n",
    "            \n",
    "            # Counters update and process progress exibited to user:\n",
    "            chunkcounter += 1\n",
    "            \n",
    "            if chunkcounter%display_step == 0:\n",
    "                print(\"{} chunks processed - {} rows processed\".format(chunkcounter, chunkcounter*chunksize))\n",
    "                \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Meriging all data...\")\n",
    "    # Merging to unique dataframe:\n",
    "    training_df = pd.concat(training_df_list, ignore_index=True)\n",
    "    test_df = pd.concat(test_df_list, ignore_index=True)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Exporting data...\")\n",
    "    # Exporting DataFrames to separated .csv files:\n",
    "    training_df.to_csv(\"training_database_{}.csv\".format(database_year), index=False)\n",
    "    test_df.to_csv(\"test_database_{}.csv\".format(database_year), index=False)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "8000\n",
      "2000\n",
      "8000\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-0c06ae3eecaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdatabase_separation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_list_2015\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2015\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-94ef62ec9836>\u001b[0m in \u001b[0;36mdatabase_separation\u001b[1;34m(filename_list, database_year, display_step, sep, chunksize)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_chunk_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# Slicing chunk's data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtraining_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_chunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mtest_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_chunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1739\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[0;32m    206\u001b[0m                                  \u001b[1;34m\"[{types}] types\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1672\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_is_valid_list_like\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1729\u001b[0m         if (hasattr(arr, '__len__') and len(arr) and\n\u001b[0;32m   1730\u001b[0m                 (arr.max() >= l or arr.min() < -l)):\n\u001b[1;32m-> 1731\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1733\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "database_separation(filename_list_2015, 2015, chunksize=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b53d545566d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdatabase_separation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_list_2016\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2016\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-a935d8b9d317>\u001b[0m in \u001b[0;36mdatabase_separation\u001b[1;34m(filename_list, database_year, sep, chunksize)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Slicing chunk's data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mtraining_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining__chunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mtest_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_chunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1739\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[0;32m    206\u001b[0m                                  \u001b[1;34m\"[{types}] types\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1672\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_is_valid_list_like\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1729\u001b[0m         if (hasattr(arr, '__len__') and len(arr) and\n\u001b[0;32m   1730\u001b[0m                 (arr.max() >= l or arr.min() < -l)):\n\u001b[1;32m-> 1731\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1733\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "database_separation(filename_list_2016, 2016, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module pandas.core.reshape.concat:\n",
      "\n",
      "concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
      "    Concatenate pandas objects along a particular axis with optional set logic\n",
      "    along the other axes.\n",
      "    \n",
      "    Can also add a layer of hierarchical indexing on the concatenation axis,\n",
      "    which may be useful if the labels are the same (or overlapping) on\n",
      "    the passed axis number.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    objs : a sequence or mapping of Series, DataFrame, or Panel objects\n",
      "        If a dict is passed, the sorted keys will be used as the `keys`\n",
      "        argument, unless it is passed, in which case the values will be\n",
      "        selected (see below). Any None objects will be dropped silently unless\n",
      "        they are all None in which case a ValueError will be raised\n",
      "    axis : {0/'index', 1/'columns'}, default 0\n",
      "        The axis to concatenate along\n",
      "    join : {'inner', 'outer'}, default 'outer'\n",
      "        How to handle indexes on other axis(es)\n",
      "    join_axes : list of Index objects\n",
      "        Specific indexes to use for the other n - 1 axes instead of performing\n",
      "        inner/outer set logic\n",
      "    ignore_index : boolean, default False\n",
      "        If True, do not use the index values along the concatenation axis. The\n",
      "        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n",
      "        concatenating objects where the concatenation axis does not have\n",
      "        meaningful indexing information. Note the index values on the other\n",
      "        axes are still respected in the join.\n",
      "    keys : sequence, default None\n",
      "        If multiple levels passed, should contain tuples. Construct\n",
      "        hierarchical index using the passed keys as the outermost level\n",
      "    levels : list of sequences, default None\n",
      "        Specific levels (unique values) to use for constructing a\n",
      "        MultiIndex. Otherwise they will be inferred from the keys\n",
      "    names : list, default None\n",
      "        Names for the levels in the resulting hierarchical index\n",
      "    verify_integrity : boolean, default False\n",
      "        Check whether the new concatenated axis contains duplicates. This can\n",
      "        be very expensive relative to the actual data concatenation\n",
      "    copy : boolean, default True\n",
      "        If False, do not copy data unnecessarily\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    concatenated : object, type of objs\n",
      "        When concatenating all ``Series`` along the index (axis=0), a\n",
      "        ``Series`` is returned. When ``objs`` contains at least one\n",
      "        ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n",
      "        the columns (axis=1), a ``DataFrame`` is returned.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The keys, levels, and names arguments are all optional.\n",
      "    \n",
      "    A walkthrough of how this method fits in with other tools for combining\n",
      "    pandas objects can be found `here\n",
      "    <http://pandas.pydata.org/pandas-docs/stable/merging.html>`__.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Series.append\n",
      "    DataFrame.append\n",
      "    DataFrame.join\n",
      "    DataFrame.merge\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Combine two ``Series``.\n",
      "    \n",
      "    >>> s1 = pd.Series(['a', 'b'])\n",
      "    >>> s2 = pd.Series(['c', 'd'])\n",
      "    >>> pd.concat([s1, s2])\n",
      "    0    a\n",
      "    1    b\n",
      "    0    c\n",
      "    1    d\n",
      "    dtype: object\n",
      "    \n",
      "    Clear the existing index and reset it in the result\n",
      "    by setting the ``ignore_index`` option to ``True``.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], ignore_index=True)\n",
      "    0    a\n",
      "    1    b\n",
      "    2    c\n",
      "    3    d\n",
      "    dtype: object\n",
      "    \n",
      "    Add a hierarchical index at the outermost level of\n",
      "    the data with the ``keys`` option.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], keys=['s1', 's2',])\n",
      "    s1  0    a\n",
      "        1    b\n",
      "    s2  0    c\n",
      "        1    d\n",
      "    dtype: object\n",
      "    \n",
      "    Label the index keys you create with the ``names`` option.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], keys=['s1', 's2'],\n",
      "    ...           names=['Series name', 'Row ID'])\n",
      "    Series name  Row ID\n",
      "    s1           0         a\n",
      "                 1         b\n",
      "    s2           0         c\n",
      "                 1         d\n",
      "    dtype: object\n",
      "    \n",
      "    Combine two ``DataFrame`` objects with identical columns.\n",
      "    \n",
      "    >>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n",
      "    ...                    columns=['letter', 'number'])\n",
      "    >>> df1\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    >>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n",
      "    ...                    columns=['letter', 'number'])\n",
      "    >>> df2\n",
      "      letter  number\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    >>> pd.concat([df1, df2])\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    \n",
      "    Combine ``DataFrame`` objects with overlapping columns\n",
      "    and return everything. Columns outside the intersection will\n",
      "    be filled with ``NaN`` values.\n",
      "    \n",
      "    >>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n",
      "    ...                    columns=['letter', 'number', 'animal'])\n",
      "    >>> df3\n",
      "      letter  number animal\n",
      "    0      c       3    cat\n",
      "    1      d       4    dog\n",
      "    >>> pd.concat([df1, df3])\n",
      "      animal letter  number\n",
      "    0    NaN      a       1\n",
      "    1    NaN      b       2\n",
      "    0    cat      c       3\n",
      "    1    dog      d       4\n",
      "    \n",
      "    Combine ``DataFrame`` objects with overlapping columns\n",
      "    and return only those that are shared by passing ``inner`` to\n",
      "    the ``join`` keyword argument.\n",
      "    \n",
      "    >>> pd.concat([df1, df3], join=\"inner\")\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    \n",
      "    Combine ``DataFrame`` objects horizontally along the x axis by\n",
      "    passing in ``axis=1``.\n",
      "    \n",
      "    >>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n",
      "    ...                    columns=['animal', 'name'])\n",
      "    >>> pd.concat([df1, df4], axis=1)\n",
      "      letter  number  animal    name\n",
      "    0      a       1    bird   polly\n",
      "    1      b       2  monkey  george\n",
      "    \n",
      "    Prevent the result from including duplicate index values with the\n",
      "    ``verify_integrity`` option.\n",
      "    \n",
      "    >>> df5 = pd.DataFrame([1], index=['a'])\n",
      "    >>> df5\n",
      "       0\n",
      "    a  1\n",
      "    >>> df6 = pd.DataFrame([2], index=['a'])\n",
      "    >>> df6\n",
      "       0\n",
      "    a  2\n",
      "    >>> pd.concat([df5, df6], verify_integrity=True)\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    ValueError: Indexes have overlapping values: ['a']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function append in module pandas.core.frame:\n",
      "\n",
      "append(self, other, ignore_index=False, verify_integrity=False)\n",
      "    Append rows of `other` to the end of this frame, returning a new\n",
      "    object. Columns not in this frame are added as new columns.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : DataFrame or Series/dict-like object, or list of these\n",
      "        The data to append.\n",
      "    ignore_index : boolean, default False\n",
      "        If True, do not use the index labels.\n",
      "    verify_integrity : boolean, default False\n",
      "        If True, raise ValueError on creating index with duplicates.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    appended : DataFrame\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    If a list of dict/series is passed and the keys are all contained in\n",
      "    the DataFrame's index, the order of the columns in the resulting\n",
      "    DataFrame will be unchanged.\n",
      "    \n",
      "    Iteratively appending rows to a DataFrame can be more computationally\n",
      "    intensive than a single concatenate. A better solution is to append\n",
      "    those rows to a list and then concatenate the list with the original\n",
      "    DataFrame all at once.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    pandas.concat : General function to concatenate DataFrame, Series\n",
      "        or Panel objects\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
      "    >>> df\n",
      "       A  B\n",
      "    0  1  2\n",
      "    1  3  4\n",
      "    >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n",
      "    >>> df.append(df2)\n",
      "       A  B\n",
      "    0  1  2\n",
      "    1  3  4\n",
      "    0  5  6\n",
      "    1  7  8\n",
      "    \n",
      "    With `ignore_index` set to True:\n",
      "    \n",
      "    >>> df.append(df2, ignore_index=True)\n",
      "       A  B\n",
      "    0  1  2\n",
      "    1  3  4\n",
      "    2  5  6\n",
      "    3  7  8\n",
      "    \n",
      "    The following, while not recommended methods for generating DataFrames,\n",
      "    show two ways to generate a DataFrame from multiple data sources.\n",
      "    \n",
      "    Less efficient:\n",
      "    \n",
      "    >>> df = pd.DataFrame(columns=['A'])\n",
      "    >>> for i in range(5):\n",
      "    ...     df = df.append({'A': i}, ignore_index=True)\n",
      "    >>> df\n",
      "       A\n",
      "    0  0\n",
      "    1  1\n",
      "    2  2\n",
      "    3  3\n",
      "    4  4\n",
      "    \n",
      "    More efficient:\n",
      "    \n",
      "    >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],\n",
      "    ...           ignore_index=True)\n",
      "       A\n",
      "    0  0\n",
      "    1  1\n",
      "    2  2\n",
      "    3  3\n",
      "    4  4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome das colunas:\n",
    "* `SG_UF_RESIDENCIA`: Sigla da Unidade de Federação da residência do candidato\n",
    "* `NU_IDADE`: Idade do candidato\n",
    "* `TP_SEXO`: Sexo do candidato\n",
    "* `TP_ESTADO_CIIVL`: Estado Civil do candidato\n",
    "* `TP_COR_RACA`: Grupo étinco a qual o candidato se identifica\n",
    "* `TP_ST_CONCLUSAO`: Situação de conclusão do Ensino Médio\n",
    "* `TP_ESCOLA`: Tipo de escola do Ensino Médio\n",
    "* `NU_NOTA`: Notas nas avaliações do ENEM, sendo:\n",
    "\n",
    "|Sigla   |    Descrição        |\n",
    "|--------|---------------------|\n",
    "|CN      |Ciências da Natureza |\n",
    "|CH      |Ciências Humanas     |\n",
    "|LC      |Linguagens e Códigos |\n",
    "|MT      |Matemática           |\n",
    "|REDACAO |Redação              |\n",
    "    \n",
    "* `Q006`: Renda mensal da família"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porquê essas colunas foram escolhidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Unidade de Federação é bem relevante no acesso à escolaridade, pois sabe-se que, no Brasil, educação de boa qualidade, de maneira geral, é acessada por meios privados. Sendo assim, há regiões que são pobres em desenvolvimento humano.\n",
    "- Idade influencia, tendo em vista que um jovem de 17-20 anos tende a lembrar mais do conteudo ante a um adulto de 30+\n",
    "- Grupo Étnico (vulgarmente conhecido como \"Raça\") tem grande peso na nota, tendo em vista o sistema de cotas\n",
    "- Situação de conclusão do Ensino Médio diz, aproximadamente, se o candidato se encontra preparado para a prova, ou não\n",
    "- Tipo de escola nos ajuda a montar o perfil do aluno, e  o que esperar do seu desempenho\n",
    "- Nota da prova é útil para a análise pra estabelecer a base de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise descritiva:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gráficos de como cada variável afeta na nota do candidato\n",
    "    - Boxplot de renda com nota\n",
    "    - Gráfico de barras para nota com outra variável qualitativa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites consultados:\n",
    "* https://openpyxl.readthedocs.io/en/2.5/pandas.html\n",
    "* http://portal.inep.gov.br/microdados\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
