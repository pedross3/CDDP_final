{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciência dos Dados - Projeto 03 - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Arthur\\space Alegro \\space de \\space Oliveira$\n",
    "\n",
    "$Pedro\\space dos\\space Santos \\space e \\space Silva$\n",
    "\n",
    "$Jhonata\\space Ferreira\\space de \\space Souza$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from openpyxl import load_workbook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *\n",
    "import warnings\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de dados do ENEM referente aos anos de 2015 e 2016 extraídas pelo site http://portal.inep.gov.br/microdados.\n",
    "\n",
    "Nomes dos arquivos:\n",
    "* MICRODADOS_ENEM_2015.csv\n",
    "* MICRODADOS_ENEM_2016.csv\n",
    "\n",
    "**OBS.:** Para rodar corretamente este arquivo arquivo iPython Notebook (`.ipynb`)  deve-se extrair o arquivo `.zip` correspondente, acessar a pasta \"DADOS\" e colocar os arquivos `.csv` citados anteriormente no mesmo diretório deste arquivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diretório:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\duals\\Documents\\GitHub\\CDDP_final\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link para a base de dados já filtrada:\n",
    "https://drive.google.com/open?id=1yq4DAJyJ2Er902X7Z1JuGc_t7w-aTIXJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Minerando Dados e Características do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando dimensões dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thousand_dot(number):\n",
    "    \"\"\"Convert int value in a string with dots as thousands separator\"\"\"\n",
    "    lista = []\n",
    "    count = 0\n",
    "    for digit in (str(number))[::-1]:\n",
    "        lista.append(digit)\n",
    "        count +=1\n",
    "        if count == 3:\n",
    "            lista.append('.')\n",
    "            count = 0\n",
    "    lista.reverse\n",
    "    elements = ''.join(lista)[::-1]\n",
    "    return elements\n",
    "\n",
    "\n",
    "def dimension_calculator(filename, chunksize=1000, sep=','):\n",
    "    \"\"\"Counts the amount of rows and columns on a .csv file\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "    # Parameters:\n",
    "    rows = 0\n",
    "    columns_labels = ''\n",
    "    count = True\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Processing...\")\n",
    "    # Opening database and counting values:\n",
    "    for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "        rows += len(chunk['NU_INSCRICAO'])\n",
    "        if count == True:\n",
    "            columns_labels = chunk.columns.values\n",
    "            count = False\n",
    "            \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\\n\")   \n",
    "    print(\"Number of rows: {}\".format(thousand_dot(rows)))\n",
    "    print(\"Number of columns: {}\".format(len(columns_labels)))\n",
    "    print(\"Number of elements: {}\".format(thousand_dot(rows*len(columns_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Process complete\n",
      "\n",
      "Number of rows: 7.746.427\n",
      "Number of columns: 166\n",
      "Number of elements: 1.285.906.882\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2015.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete\n",
      "\n",
      "Number of rows: 8.627.367\n",
      "Number of columns: 166\n",
      "Number of elements: 1.432.142.922\n"
     ]
    }
   ],
   "source": [
    "dimension_calculator(\"MICRODADOS_ENEM_2016.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem do Database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionamos uma parte da base de dados (100.000 rows de dados) para análise em escala reduzida.\n",
    "\n",
    "Após todas as filtragens e cálculos forem concluídos, todo o processo será refeito para cada `chunk`, podendo aplicar as implementações para todo a base de dados muito mais rápido, sem a necessidade de carregar o arquivo completo de uma vez (o que pode ser impossível, pois, em alguns casos, demanda mais processamento e memória que o computador possui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(to_open_filename, database_year,iteration_times, chunksize=1000, sep=','):\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value and/or file size this process might take time to compute\")\n",
    "\n",
    "    # Counters and Status Controllers:\n",
    "    overallcounter = 0\n",
    "    chunkcounter = 0\n",
    "    filecounter = 0\n",
    "    shapecounter = [0, 0]\n",
    "    to_save_filename = \"filtered_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Opening file...\")\n",
    "    # Loading database in chunks and defining chunk size, correct enconding and reading configs:\n",
    "    for chunk in pd.read_csv(to_open_filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "\n",
    "        # Selecting relevant parameters for dismiss useless data:\n",
    "        chunk = chunk[(chunk[\"TP_PRESENCA_CN\"] == 1) & (chunk[\"TP_PRESENCA_CH\"] == 1) & \n",
    "                      (chunk[\"TP_PRESENCA_LC\"] == 1) & (chunk[\"TP_PRESENCA_MT\"] == 1) & \n",
    "                      (chunk[\"TP_STATUS_REDACAO\"] == 1) & (chunk[\"IN_TREINEIRO\"] == 0)]\n",
    "\n",
    "        # Selecting relevant parameters for fitering data:\n",
    "        chunk = chunk.loc[:, [\"SG_UF_RESIDENCIA\", \"NU_IDADE\", \"TP_SEXO\", \"TP_ESTADO_CIVIL\", \"TP_COR_RACA\", \"TP_ST_CONCLUSAO\", \n",
    "                              \"TP_ESCOLA\", \"NU_NOTA_CN\", \"NU_NOTA_CH\", \"NU_NOTA_LC\", \"NU_NOTA_MT\", \"NU_NOTA_REDACAO\", \"Q006\"]]\n",
    "\n",
    "        # Exporting dataframe to new .csv file below existent data:\n",
    "        chunk.to_csv(to_save_filename, index=False)\n",
    "\n",
    "        # Counters update and process progress exibited to user:\n",
    "        chunkcounter += 1\n",
    "        overallcounter += 1\n",
    "        filecounter += 1\n",
    "        shapecounter[0] += chunk.shape[0]\n",
    "        shapecounter[1] = chunk.shape[1]\n",
    "        print(\"Processing... ({}/{}) - {} rows processed - Saving in '{}')\".format(overallcounter, iteration_times, shapecounter[0], to_save_filename))\n",
    "        to_save_filename = \"filtered_dataframe_{}({}).csv\".format(database_year, filecounter)\n",
    "    print(\"Process Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "Processing... (1/8) - 723612 rows processed - Saving in 'filtered_dataframe_2015(0).csv')\n",
      "Processing... (2/8) - 1425321 rows processed - Saving in 'filtered_dataframe_2015(1).csv')\n",
      "Processing... (3/8) - 2097387 rows processed - Saving in 'filtered_dataframe_2015(2).csv')\n",
      "Processing... (4/8) - 2742262 rows processed - Saving in 'filtered_dataframe_2015(3).csv')\n",
      "Processing... (5/8) - 3346414 rows processed - Saving in 'filtered_dataframe_2015(4).csv')\n",
      "Processing... (6/8) - 3928083 rows processed - Saving in 'filtered_dataframe_2015(5).csv')\n",
      "Processing... (7/8) - 4429373 rows processed - Saving in 'filtered_dataframe_2015(6).csv')\n",
      "Processing... (8/8) - 4768463 rows processed - Saving in 'filtered_dataframe_2015(7).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2015.csv', database_year=2015, chunksize=1000000, iteration_times=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value and/or file size this process might take time to compute\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "Processing... (1/9) - 658887 rows processed - Saving in 'filtered_dataframe_2016(0).csv')\n",
      "Processing... (2/9) - 1309576 rows processed - Saving in 'filtered_dataframe_2016(1).csv')\n",
      "Processing... (3/9) - 1932155 rows processed - Saving in 'filtered_dataframe_2016(2).csv')\n",
      "Processing... (4/9) - 2540238 rows processed - Saving in 'filtered_dataframe_2016(3).csv')\n",
      "Processing... (5/9) - 3116716 rows processed - Saving in 'filtered_dataframe_2016(4).csv')\n",
      "Processing... (6/9) - 3656028 rows processed - Saving in 'filtered_dataframe_2016(5).csv')\n",
      "Processing... (7/9) - 4154448 rows processed - Saving in 'filtered_dataframe_2016(6).csv')\n",
      "Processing... (8/9) - 4604923 rows processed - Saving in 'filtered_dataframe_2016(7).csv')\n",
      "Processing... (9/9) - 4868906 rows processed - Saving in 'filtered_dataframe_2016(8).csv')\n",
      "Process Completed\n"
     ]
    }
   ],
   "source": [
    "# Creating filtered .csv file:\n",
    "save_df('MICRODADOS_ENEM_2016.csv', database_year=2016, chunksize=1000000, iteration_times=9, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando integridade dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list_2015 = [\"filtered_dataframe_2015(0).csv\", \"filtered_dataframe_2015(1).csv\", \n",
    "                      \"filtered_dataframe_2015(2).csv\", \"filtered_dataframe_2015(3).csv\", \n",
    "                      \"filtered_dataframe_2015(4).csv\", \"filtered_dataframe_2015(5).csv\", \n",
    "                      \"filtered_dataframe_2015(6).csv\", \"filtered_dataframe_2015(7).csv\"]\n",
    "\n",
    "filename_list_2016 = [\"filtered_dataframe_2016(0).csv\", \"filtered_dataframe_2016(1).csv\",\n",
    "                      \"filtered_dataframe_2016(2).csv\", \"filtered_dataframe_2016(3).csv\", \n",
    "                      \"filtered_dataframe_2016(4).csv\", \"filtered_dataframe_2016(5).csv\", \n",
    "                      \"filtered_dataframe_2016(6).csv\", \"filtered_dataframe_2016(7).csv\", \n",
    "                      \"filtered_dataframe_2016(8).csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrity_verification(filename_list, database_year):\n",
    "    \"\"\"Counts the amount of rows in multiple .csv files\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on files amount and/or files size this process might take time to compute\")\n",
    "    rows_count = 0\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=1000, encoding='latin-1', header=0, sep=','):\n",
    "            rows_count += chunk.shape[0]\n",
    "    print(\"{} database - Total rows: {}\".format(database_year, rows_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 database - Total rows: 4768463\n",
      "2016 database - Total rows: 4868906\n"
     ]
    }
   ],
   "source": [
    "integrity_verification(filename_list_2015, 2015)\n",
    "integrity_verification(filename_list_2016, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_multifiles(filename_list, database_year):\n",
    "    \"\"\"Merging all data to unique filtered file\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\")\n",
    "\n",
    "    print(\"Process iniciated...\")\n",
    "    #\n",
    "    chunk_list = []\n",
    "    \n",
    "    print(\"Processing...\")\n",
    "    \n",
    "    for filename in filename_list:\n",
    "            for chunk in pd.read_csv(filename, chunksize=10000, encoding='latin-1', header=0, sep=','):\n",
    "                chunk_list.append(chunk)\n",
    "                \n",
    "    print(\"Merging all data...\")\n",
    "    final_df = pd.concat(chunk_list)\n",
    "    \n",
    "    print(\"Exporting data...\")\n",
    "    final_df.to_csv(\"filtered_unique_database_{}.csv\".format(database_year), index=False)\n",
    "    print(\"Process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "Processing...\n",
      "Merging all data...\n",
      "Exporting data...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "merge_multifiles(filename_list_2015, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "Processing...\n",
      "Merging all data...\n",
      "Exporting data...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "merge_multifiles(filename_list_2016, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando Bases (teste e treinamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_separation(filename_list, database_year, display_step=10, sep=',', chunksize=1000):\n",
    "    \"\"\"Generates random sample for training and test databases and exports to separated .csv files\"\"\"\n",
    "    # Raising warning:\n",
    "    warnings.warn(\"\\n\\nWARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\")\n",
    "    # Local data storages:\n",
    "    training_df_list = []\n",
    "    test_df_list = []\n",
    "    # Counter:\n",
    "    chunkcounter = 0\n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process iniciated...\")\n",
    "    # Reading all data from filtered .csv files:\n",
    "    for filename in filename_list:\n",
    "        for chunk in pd.read_csv(filename, chunksize=chunksize, encoding='latin-1', header=0, sep=sep):\n",
    "            # Creating list with chunk indexers:\n",
    "            indexers = list(range(0, (chunksize + 1)))\n",
    "            \n",
    "            #\n",
    "            chunk.reindex(indexers)\n",
    "            \n",
    "            # Random shuffling chunk's index values:\n",
    "            random.shuffle(indexers)\n",
    "            \n",
    "            # Separating 80% to training database and 20% to test database:\n",
    "            training_chunk_idx = indexers[:int((0.8)*chunksize)]\n",
    "            test_chunk_idx = indexers[int((0.8)*chunksize):]\n",
    "\n",
    "            # Sorting chunk indexers:\n",
    "            training_chunk_idx.sort()\n",
    "            test_chunk_idx.sort()\n",
    "            \n",
    "            # Slicing chunk's data:\n",
    "            training_chunk = chunk.iloc[training_chunk_idx, :]\n",
    "            test_chunk = chunk.iloc[test_chunk_idx, :]\n",
    "\n",
    "            # Setting 'is_copy' argument (avoid file corruption):\n",
    "            training_chunk.is_copy\n",
    "            test_chunk.is_copy\n",
    "            \n",
    "            # Appending separated chunk data to reference DataFrame list:\n",
    "            training_df_list.append(training_chunk)\n",
    "            test_df_list.append(test_chunk)\n",
    "            \n",
    "            # Counters update and process progress exibited to user:\n",
    "            chunkcounter += 1\n",
    "            \n",
    "            if chunkcounter%display_step == 0:\n",
    "                print(\"{} chunks processed - {} rows processed\".format(chunkcounter, chunkcounter*chunksize))\n",
    "                \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Meriging all data...\")\n",
    "    # Merging to unique dataframe:\n",
    "    training_df = pd.concat(training_df_list, ignore_index=True)\n",
    "    test_df = pd.concat(test_df_list, ignore_index=True)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Exporting data...\")\n",
    "    # Exporting DataFrames to separated .csv files:\n",
    "    #training_df.to_csv(\"training_database_{}.csv\".format(database_year), index=False)\n",
    "    test_df.to_csv(\"test_database_{}.csv\".format(database_year), index=False)\n",
    "    \n",
    "    # Process progress exibited to user:\n",
    "    print(\"Process complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2015**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0c06ae3eecaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdatabase_separation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_list_2015\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2015\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-a5c912bfd751>\u001b[0m in \u001b[0;36mdatabase_separation\u001b[1;34m(filename_list, database_year, display_step, sep, chunksize)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Slicing chunk's data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mtraining_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_chunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mtest_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_chunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1739\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[0;32m    206\u001b[0m                                  \u001b[1;34m\"[{types}] types\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1672\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_is_valid_list_like\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1729\u001b[0m         if (hasattr(arr, '__len__') and len(arr) and\n\u001b[0;32m   1730\u001b[0m                 (arr.max() >= l or arr.min() < -l)):\n\u001b[1;32m-> 1731\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1733\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "database_separation(filename_list_2015, 2015, chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reindex in module pandas.core.frame:\n",
      "\n",
      "reindex(self, labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None)\n",
      "    Conform DataFrame to new index with optional filling logic, placing\n",
      "    NA/NaN in locations having no value in the previous index. A new object\n",
      "    is produced unless the new index is equivalent to the current one and\n",
      "    copy=False\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    labels : array-like, optional\n",
      "        New labels / index to conform the axis specified by 'axis' to.\n",
      "    index, columns : array-like, optional (should be specified using keywords)\n",
      "        New labels / index to conform to. Preferably an Index object to\n",
      "        avoid duplicating data\n",
      "    axis : int or str, optional\n",
      "        Axis to target. Can be either the axis name ('index', 'columns')\n",
      "        or number (0, 1).\n",
      "    method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}, optional\n",
      "        method to use for filling holes in reindexed DataFrame.\n",
      "        Please note: this is only  applicable to DataFrames/Series with a\n",
      "        monotonically increasing/decreasing index.\n",
      "    \n",
      "        * default: don't fill gaps\n",
      "        * pad / ffill: propagate last valid observation forward to next\n",
      "          valid\n",
      "        * backfill / bfill: use next valid observation to fill gap\n",
      "        * nearest: use nearest valid observations to fill gap\n",
      "    \n",
      "    copy : boolean, default True\n",
      "        Return a new object, even if the passed indexes are the same\n",
      "    level : int or name\n",
      "        Broadcast across a level, matching Index values on the\n",
      "        passed MultiIndex level\n",
      "    fill_value : scalar, default np.NaN\n",
      "        Value to use for missing values. Defaults to NaN, but can be any\n",
      "        \"compatible\" value\n",
      "    limit : int, default None\n",
      "        Maximum number of consecutive elements to forward or backward fill\n",
      "    tolerance : optional\n",
      "        Maximum distance between original and new labels for inexact\n",
      "        matches. The values of the index at the matching locations most\n",
      "        satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      "    \n",
      "        Tolerance may be a scalar value, which applies the same tolerance\n",
      "        to all values, or list-like, which applies variable tolerance per\n",
      "        element. List-like includes list, tuple, array, Series, and must be\n",
      "        the same size as the index and its dtype must exactly match the\n",
      "        index's type.\n",
      "    \n",
      "        .. versionadded:: 0.17.0\n",
      "        .. versionadded:: 0.21.0 (list-like tolerance)\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    ``DataFrame.reindex`` supports two calling conventions\n",
      "    \n",
      "    * ``(index=index_labels, columns=column_labels, ...)``\n",
      "    * ``(labels, axis={'index', 'columns'}, ...)``\n",
      "    \n",
      "    We *highly* recommend using keyword arguments to clarify your\n",
      "    intent.\n",
      "    \n",
      "    Create a dataframe with some fictional data.\n",
      "    \n",
      "    >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n",
      "    >>> df = pd.DataFrame({\n",
      "    ...      'http_status': [200,200,404,404,301],\n",
      "    ...      'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n",
      "    ...       index=index)\n",
      "    >>> df\n",
      "               http_status  response_time\n",
      "    Firefox            200           0.04\n",
      "    Chrome             200           0.02\n",
      "    Safari             404           0.07\n",
      "    IE10               404           0.08\n",
      "    Konqueror          301           1.00\n",
      "    \n",
      "    Create a new index and reindex the dataframe. By default\n",
      "    values in the new index that do not have corresponding\n",
      "    records in the dataframe are assigned ``NaN``.\n",
      "    \n",
      "    >>> new_index= ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n",
      "    ...             'Chrome']\n",
      "    >>> df.reindex(new_index)\n",
      "                   http_status  response_time\n",
      "    Safari               404.0           0.07\n",
      "    Iceweasel              NaN            NaN\n",
      "    Comodo Dragon          NaN            NaN\n",
      "    IE10                 404.0           0.08\n",
      "    Chrome               200.0           0.02\n",
      "    \n",
      "    We can fill in the missing values by passing a value to\n",
      "    the keyword ``fill_value``. Because the index is not monotonically\n",
      "    increasing or decreasing, we cannot use arguments to the keyword\n",
      "    ``method`` to fill the ``NaN`` values.\n",
      "    \n",
      "    >>> df.reindex(new_index, fill_value=0)\n",
      "                   http_status  response_time\n",
      "    Safari                 404           0.07\n",
      "    Iceweasel                0           0.00\n",
      "    Comodo Dragon            0           0.00\n",
      "    IE10                   404           0.08\n",
      "    Chrome                 200           0.02\n",
      "    \n",
      "    >>> df.reindex(new_index, fill_value='missing')\n",
      "                  http_status response_time\n",
      "    Safari                404          0.07\n",
      "    Iceweasel         missing       missing\n",
      "    Comodo Dragon     missing       missing\n",
      "    IE10                  404          0.08\n",
      "    Chrome                200          0.02\n",
      "    \n",
      "    We can also reindex the columns.\n",
      "    \n",
      "    >>> df.reindex(columns=['http_status', 'user_agent'])\n",
      "               http_status  user_agent\n",
      "    Firefox            200         NaN\n",
      "    Chrome             200         NaN\n",
      "    Safari             404         NaN\n",
      "    IE10               404         NaN\n",
      "    Konqueror          301         NaN\n",
      "    \n",
      "    Or we can use \"axis-style\" keyword arguments\n",
      "    \n",
      "    >>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n",
      "               http_status  user_agent\n",
      "    Firefox            200         NaN\n",
      "    Chrome             200         NaN\n",
      "    Safari             404         NaN\n",
      "    IE10               404         NaN\n",
      "    Konqueror          301         NaN\n",
      "    \n",
      "    To further illustrate the filling functionality in\n",
      "    ``reindex``, we will create a dataframe with a\n",
      "    monotonically increasing index (for example, a sequence\n",
      "    of dates).\n",
      "    \n",
      "    >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n",
      "    >>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n",
      "    ...                    index=date_index)\n",
      "    >>> df2\n",
      "                prices\n",
      "    2010-01-01     100\n",
      "    2010-01-02     101\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04     100\n",
      "    2010-01-05      89\n",
      "    2010-01-06      88\n",
      "    \n",
      "    Suppose we decide to expand the dataframe to cover a wider\n",
      "    date range.\n",
      "    \n",
      "    >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n",
      "    >>> df2.reindex(date_index2)\n",
      "                prices\n",
      "    2009-12-29     NaN\n",
      "    2009-12-30     NaN\n",
      "    2009-12-31     NaN\n",
      "    2010-01-01     100\n",
      "    2010-01-02     101\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04     100\n",
      "    2010-01-05      89\n",
      "    2010-01-06      88\n",
      "    2010-01-07     NaN\n",
      "    \n",
      "    The index entries that did not have a value in the original data frame\n",
      "    (for example, '2009-12-29') are by default filled with ``NaN``.\n",
      "    If desired, we can fill in the missing values using one of several\n",
      "    options.\n",
      "    \n",
      "    For example, to backpropagate the last valid value to fill the ``NaN``\n",
      "    values, pass ``bfill`` as an argument to the ``method`` keyword.\n",
      "    \n",
      "    >>> df2.reindex(date_index2, method='bfill')\n",
      "                prices\n",
      "    2009-12-29     100\n",
      "    2009-12-30     100\n",
      "    2009-12-31     100\n",
      "    2010-01-01     100\n",
      "    2010-01-02     101\n",
      "    2010-01-03     NaN\n",
      "    2010-01-04     100\n",
      "    2010-01-05      89\n",
      "    2010-01-06      88\n",
      "    2010-01-07     NaN\n",
      "    \n",
      "    Please note that the ``NaN`` value present in the original dataframe\n",
      "    (at index value 2010-01-03) will not be filled by any of the\n",
      "    value propagation schemes. This is because filling while reindexing\n",
      "    does not look at dataframe values, but only compares the original and\n",
      "    desired indexes. If you do want to fill in the ``NaN`` values present\n",
      "    in the original dataframe, use the ``fillna()`` method.\n",
      "    \n",
      "    See the :ref:`user guide <basics.reindexing>` for more.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    reindexed : DataFrame\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.reindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2016**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duals\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "WARNING! Depending on chunksize value, files amount and/or files size this process might take time to compute\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process iniciated...\n",
      "Meriging all data...\n",
      "Exporting data...\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "database_separation(filename_list_2016, 2016, chunksize=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome das colunas:\n",
    "* `SG_UF_RESIDENCIA`: Sigla da Unidade de Federação da residência do candidato\n",
    "* `NU_IDADE`: Idade do candidato\n",
    "* `TP_SEXO`: Sexo do candidato\n",
    "* `TP_ESTADO_CIIVL`: Estado Civil do candidato\n",
    "* `TP_COR_RACA`: Grupo étinco a qual o candidato se identifica\n",
    "* `TP_ST_CONCLUSAO`: Situação de conclusão do Ensino Médio\n",
    "* `TP_ESCOLA`: Tipo de escola do Ensino Médio\n",
    "* `NU_NOTA`: Notas nas avaliações do ENEM, sendo:\n",
    "\n",
    "|Sigla   |    Descrição        |\n",
    "|--------|---------------------|\n",
    "|CN      |Ciências da Natureza |\n",
    "|CH      |Ciências Humanas     |\n",
    "|LC      |Linguagens e Códigos |\n",
    "|MT      |Matemática           |\n",
    "|REDACAO |Redação              |\n",
    "    \n",
    "* `Q006`: Renda mensal da família"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porquê essas colunas foram escolhidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Unidade de Federação é bem relevante no acesso à escolaridade, pois sabe-se que, no Brasil, educação de boa qualidade, de maneira geral, é acessada por meios privados. Sendo assim, há regiões que são pobres em desenvolvimento humano.\n",
    "- Idade influencia, tendo em vista que um jovem de 17-20 anos tende a lembrar mais do conteudo ante a um adulto de 30+\n",
    "- Grupo Étnico (vulgarmente conhecido como \"Raça\") tem grande peso na nota, tendo em vista o sistema de cotas\n",
    "- Situação de conclusão do Ensino Médio diz, aproximadamente, se o candidato se encontra preparado para a prova, ou não\n",
    "- Tipo de escola nos ajuda a montar o perfil do aluno, e  o que esperar do seu desempenho\n",
    "- Nota da prova é útil para a análise pra estabelecer a base de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise descritiva:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gráficos de como cada variável afeta na nota do candidato\n",
    "    - Boxplot de renda com nota\n",
    "    - Gráfico de barras para nota com outra variável qualitativa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites consultados:\n",
    "* https://openpyxl.readthedocs.io/en/2.5/pandas.html\n",
    "* http://portal.inep.gov.br/microdados\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
